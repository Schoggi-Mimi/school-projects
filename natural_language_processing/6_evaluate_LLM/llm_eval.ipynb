{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluate an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluate large language models (LLMs)\n",
    "    * E.g. ChatGPT, Bard, Bing Chat, HuggingChat, LLM Arena (can\n",
    "run 2 models at the same time), perplexity.ai\n",
    "* API use: Query an API of a LLaMA-2 model that we run at HSLU\n",
    "    * Details in the guest lecture tomorrow\n",
    "* Each team member evaluates 20 examples from the test set\n",
    "* Aggregate the results\n",
    "* Compare to your own trained models\n",
    "* Compare to the LLM's scores on the ARC benchmark, if you find\n",
    "published results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(\"ai2_arc\", 'ARC-Easy', split='test').to_pandas()\n",
    "#test_c = load_dataset(\"ai2_arc\", 'ARC-Challenge', split='test').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test.sample(80, random_state=42)\n",
    "#test_c_sample = test_c.sample(40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_sample[test_sample.choices.apply(lambda x: len(x.get('text')) == 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Sample indices to ensure reproducibility on different builds\n",
    "\n",
    "test_sample: `[2055, 1813,  100, 1349,   56, 1743, 1307,  611, 1058, 1995,  464, 1490,\n",
    "       2288, 1110, 1993, 1014,  695, 1748, 1414, 2196, 2128,  532, 1753, 1283,\n",
    "        889,  211,  282,  942,  237, 1754, 1907,  471, 1836, 1626,  218,  252,\n",
    "        788, 2262, 1605, 2103, 2348,  479,  602, 1087,  440, 2230, 2039, 1054,\n",
    "        618, 1736,  361, 1221, 2362, 1706,  179,  915, 1247, 1780, 1112, 1770,\n",
    "       1619, 1037,  254, 1178,  432, 1106,  729,  755, 2340,  507,  296,  621,\n",
    "       1193,  229, 1752, 1285,  857, 1518, 2024,   44]`\n",
    "\n",
    "test_c_sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(query):\n",
    "    \"\"\"\n",
    "    Returns question in a human readable format\n",
    "    \"\"\"\n",
    "    return query.question + '\\n' +'\\n'.join(f'({i[0]}) {i[1]}' for i in zip(query.choices.get('label'),query.choices.get('text')))\n",
    "\n",
    "def get_sol(query):\n",
    "    \"\"\"\n",
    "    Returns the corresponding answerKey\n",
    "    \"\"\"\n",
    "    return query.answerKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Query Llama-2 (HSLU Instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API currently not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Query Llama-2 manually (llama2.ai by Replicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings(Default):\n",
    "* Model: Llama 2 70B\n",
    "* System Prompt: \"You are a helpful assistant.\"\n",
    "* Temperature: 0.75\n",
    "* Max Tokens: 800\n",
    "* Top P: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pth = 'manual_protocol_llama2.txt'\n",
    "\n",
    "if os.path.isfile(f_pth):\n",
    "    raise FileExistsError(\"Protocol already exists. Delete first to prevent accidental overwrite. \")\n",
    "\n",
    "with open(f_pth,'w') as f:\n",
    "    for idx,i in test_sample.iterrows():\n",
    "        print(get_prompt(i) + f'\\nE{idx}\\n' + '\\tResponse:\\n' + f'\\tSolution: {get_sol(i)}\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
