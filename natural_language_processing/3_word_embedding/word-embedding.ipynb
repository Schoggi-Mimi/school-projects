{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qr \"../requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import word_embedding_classifier as model\n",
    "import word_embedding_processor as processor\n",
    "import word_embedding_evaluation as evaluation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "\n",
    "import optuna\n",
    "\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('ai2_arc', 'ARC-Easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_stopwords_fastText, y_train_with_stopwords_fastText = processor.process_dataset(dataset['train'],remove_stop_words=False, embedding_model='fasttext')\n",
    "X_val_with_stopwords_fastText, y_val_with_stopwords_fastText = processor.process_dataset(dataset['validation'], remove_stop_words=False, embedding_model='fasttext')\n",
    "X_train_without_stopwords_fastText, y_train_without_stopwords_fastText = processor.process_dataset(dataset['train'], remove_stop_words=True, embedding_model='fasttext')\n",
    "X_val_without_stopwords_fastText, y_val_without_stopwords_fastText = processor.process_dataset(dataset['validation'], remove_stop_words=True, embedding_model='fasttext')\n",
    "\n",
    "X_train_with_stopwords_word2vec, y_train_with_stopwords_word2vec = processor.process_dataset(dataset['train'],remove_stop_words=False, embedding_model='word2vec')\n",
    "X_val_with_stopwords_word2vec, y_val_with_stopwords_word2vec = processor.process_dataset(dataset['validation'], remove_stop_words=False, embedding_model='word2vec')\n",
    "X_train_without_stopwords_word2vec, y_train_without_stopwords_word2vec = processor.process_dataset(dataset['train'], remove_stop_words=True, embedding_model='word2vec')\n",
    "X_val_without_stopwords_word2vec, y_val_without_stopwords_word2vec = processor.process_dataset(dataset['validation'], remove_stop_words=True, embedding_model='word2vec')\n",
    "\n",
    "X_train_with_stopwords_glove, y_train_with_stopwords_glove = processor.process_dataset(dataset['train'],remove_stop_words=False, embedding_model='glove')\n",
    "X_val_with_stopwords_glove, y_val_with_stopwords_glove = processor.process_dataset(dataset['validation'], remove_stop_words=False, embedding_model='glove')\n",
    "X_train_without_stopwords_glove, y_train_without_stopwords_glove = processor.process_dataset(dataset['train'], remove_stop_words=True, embedding_model='glove')\n",
    "X_val_without_stopwords_glove, y_val_without_stopwords_glove = processor.process_dataset(dataset['validation'], remove_stop_words=True, embedding_model='glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Length given train dataset: {len(dataset[\"train\"])}')\n",
    "#print(f'Length processed train dataset: {len(X_train_with_stopwords)}')\n",
    "#print()\n",
    "#print(f'Length given test dataset: {len(dataset[\"validation\"])}')\n",
    "#print(f'Length processed test dataset: {len(X_val_with_stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_with_stopwords_fastText = TensorDataset(torch.tensor(X_train_with_stopwords_fastText).to(device), torch.tensor(y_train_with_stopwords_fastText).to(device))\n",
    "train_dataloader_with_stopwords_fastText = DataLoader(train_dataset_with_stopwords_fastText, batch_size=len(train_dataset_with_stopwords_fastText), shuffle=True)\n",
    "\n",
    "train_dataset_without_stopwords_FastText = TensorDataset(torch.tensor(X_train_without_stopwords_fastText).to(device), torch.tensor(y_train_without_stopwords_fastText).to(device))\n",
    "train_dataloader_without_stopwords_fastText = DataLoader(train_dataset_without_stopwords_FastText, batch_size=len(train_dataset_without_stopwords_FastText), shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_with_stopwords_word2vec = TensorDataset(torch.tensor(X_train_with_stopwords_word2vec).to(device), torch.tensor(y_train_with_stopwords_word2vec).to(device))\n",
    "train_dataloader_with_stopwords_word2vec = DataLoader(train_dataset_with_stopwords_word2vec, batch_size=len(train_dataset_with_stopwords_word2vec), shuffle=True)\n",
    "\n",
    "train_dataset_without_stopwords_word2vec = TensorDataset(torch.tensor(X_train_without_stopwords_word2vec).to(device), torch.tensor(y_train_without_stopwords_word2vec).to(device))\n",
    "train_dataloader_without_stopwords_word2vec = DataLoader(train_dataset_without_stopwords_word2vec, batch_size=len(train_dataset_without_stopwords_word2vec), shuffle=True)\n",
    "\n",
    "\n",
    "train_dataset_with_stopwords_glove = TensorDataset(torch.tensor(X_train_with_stopwords_glove).to(device), torch.tensor(y_train_with_stopwords_glove).to(device))\n",
    "train_dataloader_with_stopwords_glove = DataLoader(train_dataset_with_stopwords_glove, batch_size=len(train_dataset_with_stopwords_glove), shuffle=True)\n",
    "\n",
    "train_dataset_without_stopwords_glove = TensorDataset(torch.tensor(X_train_without_stopwords_glove).to(device), torch.tensor(y_train_without_stopwords_glove).to(device))\n",
    "train_dataloader_without_stopwords_glove = DataLoader(train_dataset_without_stopwords_glove, batch_size=len(train_dataset_without_stopwords_glove), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "output = torch.randn(10, 120).float()\n",
    "target = torch.randint(120, (10,)).long()\n",
    "loss = loss_fn(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"hslu-stableconfusion-nlp\")\n",
    "10 < float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    run = wandb.init(project=\"hslu-stableconfusion-nlp\")\n",
    "    # Define hyperparameters using the trial object\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 400, 5000)\n",
    "    output_dim = 4\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 6)\n",
    "    dataset_choice = trial.suggest_categorical('dataset', ['with_stopwords', 'without_stopwords'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-1, log=True)\n",
    "    activation_function_name = trial.suggest_categorical('activation_function', ['relu', 'sigmoid', 'tanh'])\n",
    "    word_embedding = trial.suggest_categorical('word_embedding', ['word2vec', 'fasttext'])\n",
    "    activation_function = getattr(F, activation_function_name)\n",
    "\n",
    "    wandb.config.update({\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'dataset': dataset_choice,\n",
    "        'optimizer': optimizer_name,\n",
    "        'lr': lr,\n",
    "        'activation_function': activation_function_name,\n",
    "        'embedding:': word_embedding\n",
    "    })\n",
    "\n",
    "    with open(f'fast_text_classifier_trial_{trial.number}.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Trial number', 'hidden_dim', 'num_layers', 'dataset', 'optimizer', 'lr', 'activation_function', 'word_embedding'])\n",
    "        writer.writerow([trial.number, hidden_dim, num_layers, dataset_choice, optimizer_name, lr, activation_function_name, word_embedding])\n",
    "\n",
    "    # Choose the dataset\n",
    "    if word_embedding == 'word2vec':\n",
    "        if dataset_choice == 'with_stopwords':\n",
    "            train_dataloader = train_dataloader_with_stopwords_word2vec\n",
    "            X_val = X_val_with_stopwords_word2vec\n",
    "            y_val = y_val_with_stopwords_word2vec\n",
    "        else:\n",
    "            train_dataloader = train_dataloader_without_stopwords_word2vec\n",
    "            X_val = X_val_without_stopwords_word2vec\n",
    "            y_val = y_val_without_stopwords_word2vec\n",
    "\n",
    "    elif word_embedding == 'fasttext':\n",
    "        if dataset_choice == 'with_stopwords':\n",
    "            train_dataloader = train_dataloader_with_stopwords_fastText\n",
    "            X_val = X_val_with_stopwords_fastText\n",
    "            y_val = y_val_with_stopwords_fastText\n",
    "        else:\n",
    "            train_dataloader = train_dataloader_without_stopwords_fastText\n",
    "            X_val = X_val_without_stopwords_fastText\n",
    "            y_val = y_val_without_stopwords_fastText\n",
    "    elif word_embedding == 'glove':\n",
    "        if dataset_choice == 'with_stopwords':\n",
    "            train_dataloader = train_dataloader_with_stopwords_glove\n",
    "            X_val = X_val_with_stopwords_glove\n",
    "            y_val = y_val_with_stopwords_glove\n",
    "        else:\n",
    "            train_dataloader = train_dataloader_without_stopwords_glove\n",
    "            X_val = X_val_without_stopwords_glove\n",
    "            y_val = y_val_without_stopwords_glove\n",
    "\n",
    "    # Create the model\n",
    "    net = model.Word_embedding_classifier(embedding_dim, hidden_dim, output_dim, num_layers, activation_function).to(device)\n",
    "\n",
    "    # Define your loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    val_los_exit_counter = 0\n",
    "    val_los_exit_threshold = 20\n",
    "\n",
    "    epochs_since_improvement = 0\n",
    "    improvment_limit = 100\n",
    "\n",
    "    previous_val_train_loss_divergence = float('inf')\n",
    "    epochs_since_divergence = 0\n",
    "    divergence_limit = 100\n",
    "\n",
    "    numb_epochs = 1500\n",
    "\n",
    "    complete_vall_loss = []\n",
    "  \n",
    "    for epoch in range(numb_epochs): \n",
    "        total_train_loss = 0\n",
    "        num_iterations = 0\n",
    "        sum_accuracy = 0\n",
    "\n",
    "        probress_bar = tqdm(train_dataloader)\n",
    "\n",
    "        for input, label in probress_bar:\n",
    "            label = label.type(torch.float32)\n",
    "            output = net(input[:,0], input[:,1], input[:,2], input[:,3], input[:,4])\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            sum_accuracy += evaluation.calculate_accuracy_from_predicitions(output, label)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            num_iterations += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            probress_bar.set_description('Train loss: %.6f' % (loss))\n",
    "            probress_bar.update()\n",
    "\n",
    "        accuracy = sum_accuracy / num_iterations\n",
    "\n",
    "        train_loss = total_train_loss / num_iterations\n",
    "\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": accuracy})\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_labels = torch.asarray(y_val).to(device).type(torch.float32)\n",
    "            val_inputs = torch.asarray(X_val).to(device).type(torch.float32)\n",
    "\n",
    "            val_outputs = net(val_inputs[:,0], val_inputs[:,1], val_inputs[:,2], val_inputs[:,3], val_inputs[:,4])\n",
    "\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_accuracy_score = evaluation.calculate_accuracy_from_predicitions(val_outputs, val_labels)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(net.state_dict(), f'fast_text_classifier_trial_{trial.number}.pth')\n",
    "            best_val_loss = val_loss\n",
    "            val_los_exit_counter = 0\n",
    "        elif val_loss < best_val_loss + 0.05:\n",
    "            val_los_exit_counter = 0\n",
    "        else:\n",
    "            val_los_exit_counter += 1\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "\n",
    "        if abs(val_loss - train_loss) > previous_val_train_loss_divergence:\n",
    "            epochs_since_divergence += 1\n",
    "        else:\n",
    "            epochs_since_divergence = 0\n",
    "        previous_val_train_loss_divergence = abs(val_loss - train_loss)\n",
    "        \n",
    "        wandb.log({\"Validation Loss\": val_loss.item(), \"Validation Accuracy\": val_accuracy_score})\n",
    "        complete_vall_loss.append(val_loss.item())\n",
    "        \n",
    "        if val_los_exit_counter >= val_los_exit_threshold or epochs_since_improvement >= improvment_limit or epochs_since_divergence >= divergence_limit:\n",
    "            print(\"Early stopping due to validation loss not impoving, train loss not improving or train and validation loss diverging\")\n",
    "            break\n",
    "\n",
    "        net.train()\n",
    "\n",
    "    val_losses_tensor = torch.tensor(complete_vall_loss)\n",
    "    val_losses_tensor, _ = torch.sort(val_losses_tensor)\n",
    "    lowest_10_percent = int(len(val_losses_tensor) * 0.1)\n",
    "    average_best_10_percent_val_loss = torch.mean(val_losses_tensor[:lowest_10_percent])\n",
    "    return average_best_10_percent_val_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=70)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "with open('best_config.txt', 'w') as f:\n",
    "    f.write('Best trial:\\n')\n",
    "    f.write('  Trial number: {}\\n'.format(trial.number))\n",
    "    f.write('  Value: {}\\n'.format(trial.value))\n",
    "    f.write('  Params:\\n')\n",
    "    for key, value in trial.params.items():\n",
    "        f.write('    {}: {}\\n'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
