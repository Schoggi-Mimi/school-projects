{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b057d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f4526-52bb-42c6-ae69-0be705818dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import noise_filter\n",
    "import preprocessing\n",
    "import base_model\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7926fd4",
   "metadata": {},
   "source": [
    "## Run Instructions\n",
    "\n",
    "### W&B logger\n",
    "\n",
    "The Weights&Biases API will ask for your API-Key when you run it for the first time. (get your key -> <a>https://wandb.ai/authorize</a>)\n",
    "\n",
    "Please note that your API-Key is stored in your home directory under `~/.netrc` and will automatically be used for all future runs even in other environments/projects. Use with caution on shared instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35e168-dea2-4f83-a25c-f102a6657305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e608d-768b-40c8-a27e-e5c50fb2f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = preprocessing.Preprocessor(max_sequence_length=457)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e287f8-5e4f-4bee-b01e-f5abe5194ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNA(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=256, nlayers=3):\n",
    "        super(LSTM_RNA, self).__init__()\n",
    "        self.model_config = {\"hidden_size\":hidden_size, \"nlayers\":nlayers}\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = nlayers\n",
    "        self.lstm1 = torch.nn.LSTM(input_size=1, \n",
    "                                   hidden_size=self.hidden_size, \n",
    "                                   num_layers=self.num_layers, \n",
    "                                   bidirectional=True, \n",
    "                                   batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(self.hidden_size*2, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.randn(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.randn(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        output, (hn, cn) = self.lstm1(x, (h_0, c_0))\n",
    "        \n",
    "        output = self.fc1(torch.relu(output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_lstm(suffix, epochs):\n",
    "    lstm = LSTM_RNA()\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    lstm = base_model.BaseModel(optimizer, lstm, f'LSTM-{suffix}.pth', scheduler=scheduler, enable_wandb=True)\n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df031a31-accb-4330-915d-eca1df02fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_RNA(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(RNN_RNA, self).__init__()\n",
    "        self.model_config = {\"hidden_size\":hidden_size}\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 3\n",
    "        self.rnn1 = torch.nn.RNN(input_size=1, \n",
    "                                   hidden_size=self.hidden_size, \n",
    "                                   num_layers=self.num_layers, \n",
    "                                   bidirectional=True, \n",
    "                                   batch_first=True,\n",
    "                                   nonlinearity='relu')\n",
    "        self.fc1 = torch.nn.Linear(self.hidden_size*2, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        h_0 = torch.randn(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        output, hn = self.rnn1(x, h_0)\n",
    "        \n",
    "        output = self.fc1(torch.relu(output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def create_rnn(suffix, epochs):\n",
    "    rnn = RNN_RNA()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    rnn = base_model.BaseModel(optimizer, rnn, f'RNN-{suffix}.pth', scheduler=scheduler, enable_wandb=True)\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf5b06-4cd3-402f-b166-d24f50eaa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer_RNA(torch.nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, out_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_config = {\"ntoken\":ntoken, \"d_model\":d_model, \"nhead\":nhead, \"d_hid\":d_hid, \"nlayers\":nlayers, \"out_dim\":out_dim,\"dropout\":dropout}\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, norm_first=True)\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.linear = torch.nn.Linear(d_model, out_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = src.squeeze(dim=-1)\n",
    "        mask = (src == 0)\n",
    "        src = src.permute(1, 0)\n",
    "\n",
    "        src = self.embedding(src.to(torch.int32)) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        output = self.transformer_encoder(src, src_key_padding_mask=mask)\n",
    "\n",
    "        # Apply final linear layer\n",
    "        final_output = self.linear(output)\n",
    "\n",
    "        final_output = final_output.permute(1, 0, 2)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "def create_transformer(suffix, epochs):\n",
    "    ntokens = 500  # size of vocabulary (Note: This has to be > ~500 if Structure is encoded)\n",
    "    emsize = 200  # embedding dimension\n",
    "    d_hid = 800  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "    nlayers = 6  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "    nhead = 5  # number of heads in ``nn.MultiheadAttention``\n",
    "    dropout = 0.2  # dropout probability\n",
    "    out_dim = 2 # 1 if dual model, 2 if single model\n",
    "\n",
    "    transformer = Transformer_RNA(ntokens, emsize, nhead, d_hid, nlayers, out_dim, dropout)\n",
    "    optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.001, weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    transformer = base_model.BaseModel(\n",
    "        optimizer, transformer, f'TRANSFORMER-{suffix}.pth', scheduler=scheduler, enable_wandb=True)\n",
    "    return transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72dea833-ff0c-4507-9e8c-b115450ec540",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e219fc-6def-495b-bb79-97a4f9730740",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69e5fd-bbf9-4f07-b527-126ce8814190",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_newfeat2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = testing.Testing()\n",
    "tester.run_tests(preprocessor=preprocessor, create_model=create_lstm, num_epochs=NUM_EPOCHS, train=train,\n",
    "                 split_and_fold_tests=False, filter_noise_tests=False, structure_clip_and_weighted_loss_tests=False,\n",
    "                 different_weighted_loss_tests=True, additive_weighted_loss_tests=True,\n",
    "                 additional_tags=['LSTM', '256'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306fb2c0-c292-4695-af84-ed38d0e0b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets, preprocessing_config = preprocessor.prepare_xy_split(\n",
    "    train, categorical=True, \n",
    "    shuffle=True, validation_split=None, \n",
    "    batch_size=128, filter_noise=True, \n",
    "    dual_model=False, k_fold=5, structure=True,\n",
    "    clip=True, weighted_loss=None, additive_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6ec95-f1ab-48d0-aa68-6e87c9cf9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "skip_first = False\n",
    "for experiment_type, train_data_loader, validation_data_loader in train_sets:\n",
    "    if skip_first:\n",
    "        skip_first = False\n",
    "        continue\n",
    "        \n",
    "    print(f'Model fit {experiment_type}')\n",
    "    model = create_rnn(experiment_type, NUM_EPOCHS)\n",
    "    training_losses, validation_losses = model.fit(\n",
    "        train_data_loader,\n",
    "        validation_data_loader,\n",
    "        experiment_type=experiment_type,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        preprocessing_config=preprocessing_config)\n",
    "    losses.append([training_losses, validation_losses])\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790421e-11ba-4b8d-85b7-09c563c2128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_gpu()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59368b-86d9-47de-8351-3aba7b89b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(NUM_EPOCHS), training_losses, label='Train Loss', marker='o', color='orange')\n",
    "# Plotting the validation loss\n",
    "plt.plot(range(NUM_EPOCHS), validation_losses, label='Validation Loss', marker='o', color='midnightblue')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e220b623-4ed5-4b42-9e75-09342ce3e036",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "TBD: Add structure test set and set structure=True for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c4523-502f-4bad-bcca-446f6855af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_newfeat2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca43fba-fa89-4310-8764-e89948ca42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = preprocessor.prepare_prediction_dataset(test_data, batch_size=512, categorical=True, structure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910777f-60fe-4461-9d2a-3fcc0dc01254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small_test = [a[1] for a in list(enumerate(test_data))[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d257a-a949-423c-9eef-171b8aaa3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single model mode\n",
    "import gc\n",
    "final_outputs = pd.DataFrame()\n",
    "final_outputs.index.name = 'id'\n",
    "experiment_types = ['DMS_AND_2A3_MaP']\n",
    "\n",
    "for experiment_type in experiment_types:\n",
    "    print(f'Model prediction {experiment_type}')\n",
    "    model = create_transformer(experiment_type, NUM_EPOCHS)\n",
    "    model.load_model()\n",
    "    \n",
    "    final_predictions = model.predict(test_data, single_model_mode=True)\n",
    "    print(final_predictions.shape)\n",
    "    \n",
    "    final_outputs[f'reactivity_DMS_MaP'] = final_predictions[:,0].cpu().numpy()\n",
    "    final_outputs[f'reactivity_2A3_MaP'] = final_predictions[:,1].cpu().numpy()\n",
    "    del final_predictions\n",
    "    del model\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "final_outputs.to_csv(f'Transformer_AdamW_SingleModel_{NUM_EPOCHS}Epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de00d3-8f4f-4776-9449-0042baf6b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dual model mode\n",
    "import gc\n",
    "final_outputs = pd.DataFrame()\n",
    "final_outputs.index.name = 'id'\n",
    "experiment_types = ['DMS_MaP', '2A3_MaP']\n",
    "\n",
    "for experiment_type in experiment_types:\n",
    "    print(f'Model prediction {experiment_type}')\n",
    "    model = create_transformer(experiment_type, NUM_EPOCHS)\n",
    "    model.load_model()\n",
    "    \n",
    "    final_predictions = model.predict(test_data)\n",
    "    \n",
    "    final_outputs[f'reactivity_{experiment_type}'] = final_predictions.cpu().numpy()\n",
    "    del final_predictions\n",
    "    del model\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "final_outputs.to_csv(f'Transformer_AdamW_DualModel_{NUM_EPOCHS}Epochs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf901c17-c4fa-4bcf-b3e1-d57abf07a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs.shape[0] == 269796671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888660bd-7b9d-4c15-980b-c02204049889",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b12ed8-c3d8-4d0d-af68-c95cd8b4eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs.min(), final_outputs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read first 50 lines of the submission file to check results\n",
    "pd.read_csv(f'RNN_CosineScheduler_{NUM_EPOCHS}Epochs.csv', nrows=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57fcde67-86be-4c46-8a7b-30d660c0a033",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533ae82-0b0f-4267-910e-b7e4c1046064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2ccfb72-3079-440e-9830-e60b5a853fc2",
   "metadata": {},
   "source": [
    "## Clip Submission to [0,1] (recommended)\n",
    "\n",
    "This ensures that all predictions are in the range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.read_csv('Transformer_AdamW_DualModel_v2_10Epochs.csv',index_col=0)\n",
    "subm = subm.clip(0.0,1.0)\n",
    "subm.to_csv('Transformer_AdamW_DualModel_v2_10Epochs_clipped.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001932eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Submissions\n",
    "\n",
    "sm1 = pd.read_csv(f'Transformer_AdamW_DualModel_v2_10Epochs.csv', nrows=50000)\n",
    "sm2 = pd.read_csv(f'Transformer_AdamW_DualModel_v2_10Epochs_clipped.csv', nrows=50000)\n",
    "sm1.describe()\n",
    "sm2.describe()\n",
    "\n",
    "merged_df = pd.merge(sm1, sm2, on='id', how='inner', suffixes=[\"_sm1\",\"_sm2\"])\n",
    "\n",
    "merged_df.sample(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
